# Switching autoregressive system

In this chapter, we focus on a more complex system involving autoregressive structure. We still focus on a time series $y_0, \dots, y_n$ of values in $\mathbb{R}^d$. However, it is now supposed that the time series dynamics could change through time, according to an unobserved stochastic process in a discrete space. 
This unobserved process might model different regimes of the dynamics (see @rabiner1989tutorial for selected applications).

## Model

Taking the same notations and dimensions as in equation \@ref(eq:AR-simple)
We assume that these observations are realisations of random variables $Y_0,\dots, Y_n$ such that:
\begin{align*}
Z_0 &\sim \chi_{0, Z}(\cdot)\\
Z_t \vert Z_{t - 1} &\sim p(z_t \vert Z_{t - 1}) \\
Y_0 &\sim \chi_{0, Y}(\cdot \vert Z_0), \\
Y_t \vert Z_t &= m(Z_t) + \mathbf{A}(Z_t)Y_{t -1} + E_t,~1\leq t \leq n 
\end{align*}
where 

- $\left\lbrace Z_t \right\rbrace_{0\leq t \leq n}$ is an homogeneous Markov chain taking value on the finite space $\mathbb{K} = \lbrace1,\dots, K \rbrace$, and of transition matrix denoted by $\mathbf{P}$,
- $\lbrace m(k)\in\mathbb{R}^d,~\mathbf{A}(k)\in \mathcal{M}_{d\times d}\rbrace_{k = 1,\dots, K}$ are unknown parameters.
- $\chi_{0, Z}(\cdot)$ and $\chi_{0, Y}(\cdot \vert Z_0)$ are some probability distributions over $\mathbb{K}$ and $\mathbb{R}^d$
- $p(z_t\vert Z_{t-1})$  is the law of $Z_t$ condtionnally to $Z_{t - 1}$ (here the line of $\mathbf{P}$ given by $Z_{t - 1}$).
- $E_t$ is a random vector such that: 
\begin{equation*}
E_t \overset{ind.}{\sim} \mathcal{N}\left(0, \mathbf{\Sigma}\right).
\end{equation*} where $\mathbf{\Sigma}$ is $d\times d$ covariance matrix.

In this context, the set of unknown parameters is given by 
$$\theta = \left\lbrace \mathbf{P}, m(k),~\mathbf{A}(k), \mathbf{\Sigma}\right \rbrace_{k = 1,\dots, K}.$$

## Inference

In this context, the inference task is twofold:

1. Obtaining maximum likelihood estimate of $\theta$;
2. Retracing the hidden sequence $Z_0,\dots Z_n$ given the observations $Y_{0:n}$.

It is well known that these two tasks are indeed complementary. 
A common way to solve these problems is the Expectation Maximization (EM) algorithm
[@dempster1977maximum]. 
The algorithm is shortly depicted here.

### Comment about notations

In the following, we use the notation $p$ for a generic probability distribution. The law to which it refers is explicit through arguments. For instance $p(y_{0:n} \vert z_{0:n})$ is the p.d.f. of a Gaussian vector, the random vector $Y_{0:n}\vert Z_{0:n}$, and $p(z_t\vert z_{t - 1})$ is the law of the discrete random variable $Z_{t} \vert Z_{t - 1}$ evaluated at $z_t$ and $z_{t -1}$. 
In this context, $p(z_t\vert z_{t - 1}) = \mathbb{P}(Z_t = z_t \vert \lbrace Z_{t-1} = z_{t -1}\rbrace.$

### Likelihood

A straightforward way to compute the likelihood in this model can be obtained, just using the Markov properties of this model:
\begin{align}
L(\theta \vert y_{0:n}) &:= p(y_{0:n} \vert \theta) \nonumber \\
&= \sum_{z_{0:n}} p(y_{0:n}, z_{0:n}) \nonumber \\
&= \sum_{z_{0:n}} p(z_{0:n})p(y_{0:n}\vert z_{0:n}) \nonumber \\
&=  \sum_{z_{0:n}} p(z_0 \vert \theta) p(y_0\vert z_0, \theta) \prod_{t = 1}^n p(z_{t} \vert z_{t - 1}, \theta)p(y_{t}\vert y_{t -1}, z_{t}, \theta)  (\#eq:AR-HMM-likelihood).
\end{align}
For a known $\theta$, every term in \@ref(eq:AR-HMM-likelihood) can be computed.
However, in a general setting, there exists $K^{n + 1}$ possible sequences $z_{0:n}$, which makes this direct computation hardly feasible for any common values of $n$.

### Complete log-likelihood

A workaround to find the maximum likelihood estimate is the EM algorithm. 
In this context, we focus on a different function, the *complete log-likelihood*, i.e. the likelihood of the *completed* observations (what we wish we could observe), $(y_{0:n}, x_{0:n})$, we have that:

\begin{align}
\ell(\theta \vert y_{0:n}, z_{0:n}) :=& \log p(y_{0:n}, z_{0:n} \vert \theta) \nonumber \\
=& \log p(y_{0:n}, z_{0:n}) \nonumber \\
=& \log p(z_{0:n}) + \log p(y_{0:n}\vert z_{0:n}) \nonumber \\
=&  \log p(z_0 \vert \theta) + \log p(y_0\vert z_0, \theta) \nonumber \\ 
&+ \sum_{t = 1}^n \log p(z_{t} \vert z_{t - 1}, \theta) + \sum_{t = 1}^n \log p(y_{t}\vert y_{t -1}, z_{t}, \theta)  (\#eq:AR-HMM-complete-log-likelihood).
\end{align}

For a given set of parameters, say $\theta^{(0)}$, let's consider the following function of $\theta$

\begin{align}
Q(\theta \vert \theta_0) &:= \mathbb{E}[\ell(\theta \vert Y_{0:n}, Z_{0:n}) \vert Y_{0:n} = y_{0:n}, \theta^{(0)}] \nonumber\\ 
&= \sum_{z_{0:n}} \ell(\theta \vert y_{0:n}, z_{0:n}) p(z_{0:n} \vert y_{0:n}, \theta^{(0)}) \text{d} z_{0:n} \nonumber \\
&= \sum_{k = 1}^K \left(\log p(z_0 = k) + \log(p(y_0 \vert z_0 = k))\right) +
\sum_{k = 1}^K \left(\log p(z_0 = k) + \log(p(y_0 \vert z_0 = k))\right) +
\sum_{k = 1}^K\sum_{k' = 1}^K \sum_{t = 1}^n \log p(z_{t} \vert z_{t - 1}, \theta^{0})
\ell(\theta \vert y_{0:n}, z_{0:n}) p(z_{0:n} \vert y_{0:n}, \theta^{(0)}) \text{d} z_{0:n} \nonumber
\end{align}
